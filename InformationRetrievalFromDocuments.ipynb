{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "989e4445-211b-4e7e-88ee-7f668fd1163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from getpass import getpass\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7e88b41b-d272-4957-91a6-fc406ba35eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HF Token: ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"HF Token:\")\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b9ed9-5d56-4fe6-9139-c269e766aae0",
   "metadata": {},
   "source": [
    "## Loading files\n",
    "\n",
    "The files used are the first three lecture transcripts of the Stanford CS229 course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "29e53040-0acb-476f-80fb-e255d4548374",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileLoc = \"D:\\\\github\\\\LLMS\\\\transcripts\"\n",
    "files = os.listdir(fileLoc)\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(os.path.join(fileLoc, files[0])),\n",
    "    PyPDFLoader(os.path.join(fileLoc, files[1])),\n",
    "    PyPDFLoader(os.path.join(fileLoc, files[2]))\n",
    "]\n",
    "\n",
    "# \n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cc09c2cf-3afb-476d-9091-07f845c9647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "textSplitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = textSplitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b471d-1ba2-496d-afa2-fd1af6b4ad41",
   "metadata": {},
   "source": [
    "### Loading a model for vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3a25afd4-2d60-450e-b065-d8cfb93a7029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bswat\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# model_name = \"Snowflake/snowflake-arctic-embed-m\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name = EMBEDDING_MODEL_NAME, encode_kwargs={\"normalize_embeddings\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2e1d2-f7af-4f47-8bd4-27bdb87feeba",
   "metadata": {},
   "source": [
    "Creating a persist directory and removing the files that already exist in the persist directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c54ab829-8284-46d2-a8cc-6bd7caa6d99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persist_directory = \"D:\\\\github\\\\LLMS\\\\PersistDir\"\n",
    "os.system(\"rm -rf D:\\\\github\\\\LLMS\\\\PersistDir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c58620d-b0ab-43e2-90e8-80c3539dc38b",
   "metadata": {},
   "source": [
    "### Storing the embeddings in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "952ac2b2-f00e-4c0b-86bf-7a404b53eea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1816\n"
     ]
    }
   ],
   "source": [
    "vectorDB = Chroma.from_documents(\n",
    "    documents = splits,\n",
    "    embedding = embeddings,\n",
    "    persist_directory = persist_directory\n",
    ")\n",
    "\n",
    "print(vectorDB._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb83fac-a693-447e-acae-b1269212fc5a",
   "metadata": {},
   "source": [
    "### Retriving relevant information from the stored vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "65175b75-4dea-41bf-a76a-08a1f26cf408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "### Answer Retrieval through simpler sematic search\n",
    "\n",
    "question = \"What are the names of the Course TA?\"\n",
    "\n",
    "ans = vectorDB.similarity_search(question, k = 3)\n",
    "print(len(ans))  ### The search has returned three chunks as answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "344b32df-d468-41b5-bff3-9f42ebd326f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"learning algorithms to teach a car how to  drive at reasonably high speeds off roads \\navoiding obstacles.  \\nAnd on the lower right, that's a robot program med by PhD student Eva Roshen to teach a \\nsort of somewhat strangely configured robot how to get on top of an obstacle, how to get \\nover an obstacle. Sorry. I know the video's kind of small. I hope you can sort of see it. \\nOkay?  \\nSo I think all of these are robots that I thi nk are very difficult to hand-code a controller \\nfor by learning these sorts of l earning algorithms. You can in relatively short order get a \\nrobot to do often pretty amazing things.  \\nOkay. So that was most of what I wanted to say today. Just a couple more last things, but \\nlet me just check what questions you have righ t now. So if there are no questions, I'll just \\nclose with two reminders, which are after class today or as you start to talk with other \\npeople in this class, I just encourage you again to start to form project partners, to try to \\nfind project partners to do your project with. And also, this is a good time to start forming \\nstudy groups, so either talk to your friends  or post in the newsgroup, but we just \\nencourage you to try to star t to do both of those today, okay? Form study groups, and try \\nto find two other project partners.  \\nSo thank you. I'm looking forward to teaching this class, and I'll see you in a couple of \\ndays.\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed421ff3-3cf0-414d-9959-83e75c450e93",
   "metadata": {},
   "source": [
    "The returned answer has a chunk of from the documents stored in the vector database that has been found to be related to the question based on its semantic similarity. There are some limitations to the semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "68bc4811-c9cd-4e96-ae5f-4e0f10df9cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What was told about supervised learning?\"\n",
    "\n",
    "ans = vectorDB.similarity_search(question, k = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e446879e-8e67-4781-adc9-79dbba3c4cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrived context 1\n",
      "So I just want to start by showing you a f un video. Remember at the last lecture, the \n",
      "initial lecture, I talk ed about supervised learning. And supervised learning was this \n",
      "machine-learning problem where I said we'r e going to tell the algorithm what the close \n",
      "right answer is for a number of examples, a nd then we want the algorithm to replicate \n",
      "more of the same.  \n",
      "So the example I had at the first lecture was the problem of predicting housing prices, \n",
      "where you may have a training set, and we tell the algorithm what the \"right\" housing \n",
      "price was for every house in the training set. And then you want the algorithm to learn the \n",
      "relationship between sizes of houses and the pr ices, and essentially produce more of the \n",
      "\"right\" answer.  \n",
      "So let me show you a video now. Load the bi g screen, please. So I'll show you a video \n",
      "now that was from Dean Pomerleau at some work he did at Carnegie Mellon on applied \n",
      "supervised learning to get a car to drive itself . This is work on a vehicle known as Alvin. \n",
      "It was done sort of about 15 years ago, and I think it was a very el egant example of the \n",
      "sorts of things you can get supervised or any algorithms to do.  \n",
      "On the video, you hear Dean Pomerleau's voice mention and algorithm called Neural \n",
      "Network. I'll say a little bit about that later, but the essential learni ng algorithm for this is \n",
      "something called gradient descent, which I will  talk about later in today's lecture. Let's \n",
      "watch the video. [Video plays]\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrived context 1\")\n",
    "print(ans[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98f6e7-41fa-467b-bc64-340721c7b485",
   "metadata": {},
   "source": [
    "### LLM aided Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bc19c03c-3bcf-4459-a13f-ed91bdfc8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"huggingfaceh4/zephyr-7b-alpha\", \n",
    "    model_kwargs={\"temperature\": 0.01, \"max_length\": 64,\"max_new_tokens\":512}\n",
    ")\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3046460-3025-4fb9-bf9b-6503a74507dd",
   "metadata": {},
   "source": [
    "The compressor is the extractor which uses the LLM. The entire vector database is run through the conpressing LLM and the most relevant chunks from the documnets are returned. The retrieval metric used is the **Maximum Marginal Relevance** which returns the most diverse chunks returned by the Compressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f2a0dcd9-cd86-4c3a-9a4a-3cc7b96eda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor = compressor,\n",
    "    base_retriever = vectorDB.as_retriever(search_type = \"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "192688d7-897c-4796-9c3e-f71361e2ab53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"What was told about supervised learning?\"\n",
    "\n",
    "ans = compression_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b84ae8a1-678a-44e7-818f-e214d1983351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT. \n",
      "\n",
      "Remember, *DO NOT* edit the extracted parts of the context.\n",
      "\n",
      "> Question: What was told about supervised learning?\n",
      "> Context:\n",
      ">>>\n",
      "So I just want to start by showing you a f un video. Remember at the last lecture, the \n",
      "initial lecture, I talk ed about supervised learning. And supervised learning was this \n",
      "machine-learning problem where I said we'r e going to tell the algorithm what the close \n",
      "right answer is for a number of examples, a nd then we want the algorithm to replicate \n",
      "more of the same.  \n",
      "So the example I had at the first lecture was the problem of predicting housing prices, \n",
      "where you may have a training set, and we tell the algorithm what the \"right\" housing \n",
      "price was for every house in the training set. And then you want the algorithm to learn the \n",
      "relationship between sizes of houses and the pr ices, and essentially produce more of the \n",
      "\"right\" answer.  \n",
      "So let me show you a video now. Load the bi g screen, please. So I'll show you a video \n",
      "now that was from Dean Pomerleau at some work he did at Carnegie Mellon on applied \n",
      "supervised learning to get a car to drive itself . This is work on a vehicle known as Alvin. \n",
      "It was done sort of about 15 years ago, and I think it was a very el egant example of the \n",
      "sorts of things you can get supervised or any algorithms to do.  \n",
      "On the video, you hear Dean Pomerleau's voice mention and algorithm called Neural \n",
      "Network. I'll say a little bit about that later, but the essential learni ng algorithm for this is \n",
      "something called gradient descent, which I will  talk about later in today's lecture. Let's \n",
      "watch the video. [Video plays]\n",
      ">>>\n",
      "Extracted relevant parts:\n",
      "- At the last lecture, the initial lecture, I talk ed about supervised learning.\n",
      "- Supervised learning was this machine-learning problem where I said we're going to tell the algorithm what the close right answer is for a number of examples, and then we want the algorithm to replicate more of the same.\n",
      "- The example I had at the first lecture was the problem of predicting housing prices, where you may have a training set, and we tell the algorithm what the \"right\" housing price was for every house in the training set.\n",
      "- And then you want the algorithm to learn the relationship between sizes of houses and the prices, and essentially produce more of the \"right\" answer.\n",
      "- The essential learning algorithm for this is something called gradient descent, which I will talk about later in today's lecture.\n",
      "\n",
      "Relevant output:\n",
      "- The lecture discusses supervised learning, where the algorithm is given a training set and told what the correct answer is for each example. The goal is for the algorithm to learn the relationship between the input and output and produce more of the correct answer. The lecture also mentions an example of predicting housing prices and the learning algorithm used, which is gradient descent.\n"
     ]
    }
   ],
   "source": [
    "print(ans[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
